{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a miniGPT language model with JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.kaggle.com/static/images/logos/kaggle-logo-transparent-300.png\" height=\"32\" width=\"70\"/>Run in Kaggle</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIOXoY1xgiww"
   },
   "source": [
    "This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for [Single-Program Multi-Data](https://en.wikipedia.org/wiki/Single_program,_multiple_data)). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
    "\n",
    "Here, you will learn how to:\n",
    "\n",
    "- Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "- Load and preprocess the dataset\n",
    "- Create the loss and training step functions\n",
    "- Train the model on TPUs on Kaggle or Google Colab\n",
    "- Profile for hyperparameter tuning\n",
    "\n",
    "If you are new to JAX for AI, check out the [introductory tutorial](https://jax-ai-stack.readthedocs.io/en/latest/neural_net_basics.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTmz5Cbco7n_"
   },
   "source": [
    "## Setup\n",
    "\n",
    "JAX installation is covered in [this guide](https://jax.readthedocs.io/en/latest/installation.html) on the JAX documentation site. We will use [Tiktoken](https://github.com/openai/tiktoken) for tokenization and [Grain](https://google-grain.readthedocs.io/en/latest/index.html) for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zMsOIc7ouCO",
    "outputId": "037d56a9-b18f-4504-f80a-3a4fa2945068"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq \"jax[tpu]\" pandas tiktoken jax-ai-stack[grain] matplotlib safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHzJ_bokoovZ"
   },
   "source": [
    "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUjQsgQEmI1N",
    "outputId": "e6eff24e-5578-4277-a0f9-24e27bd91ee0"
   },
   "outputs": [],
   "source": [
    "!gcloud storage cp gs://jk-us-public/TinyStories-train.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rcji_799n4eA"
   },
   "source": [
    "**Note:** If you are using [Kaggle](https://www.kaggle.com/), select the free TPU v5e-8 as the hardware accelerator. If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v5e-1 as the hardware accelerator. You may also use Google Cloud TPUs.\n",
    "\n",
    "Check the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS9sQEY3n0mB",
    "outputId": "9ffcf3a6-20ef-4f80-b006-f5d3c5644a15"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKE2uUafLobI"
   },
   "source": [
    "Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKYFNOhdLq98"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding # For data and model parallelism (explained in more detail later)\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import grain.python as pygrain\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPyt7MV6prz1"
   },
   "source": [
    "## Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "\n",
    "### Leveraging JAX's data and tensor parallelism\n",
    "\n",
    "One of the most powerful features of JAX is [device parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for SPMD.\n",
    "\n",
    "- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n",
    "- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n",
    "- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n",
    "\n",
    "In this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup, which is aligned with Kaggle TPU v5e-8 or newer GCP TPUs chips.\n",
    "\n",
    "Note that as of October 2025, free-tier Colab only offers TPU v5e-1, which can no longer support SPMD.\n",
    "\n",
    "### jax.sharding.Mesh\n",
    "\n",
    "Earlier, we imported [`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) - is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n",
    "\n",
    "Our `Mesh` will have two arguments:\n",
    "- `devices`: This will take the value of [`jax.experimental.mesh_utils((4, 2))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n",
    "- `axis_names`, where:\n",
    "  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n",
    "  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor parallism\n",
    "\n",
    "This matches the structure in the Kaggle TPU v5e setup.\n",
    "\n",
    "Let's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuMlCK3Q8WJD"
   },
   "outputs": [],
   "source": [
    "# Create a `Mesh` object representing TPU device arrangement.\n",
    "# For example, for Kaggle TPU v5e-8:\n",
    "if jax.device_count() == 8:\n",
    "    mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "\n",
    "    ### Alternatively, we could use the 8-way data parallelism with only one line of code change.\n",
    "    ### JAX enables quick experimentation with different partitioning strategies\n",
    "    ### like this. We will come back to this point at the end of this tutorial.\n",
    "    # mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
    "\n",
    "### For free-tier Colab TPU, which only has a single TPU core\n",
    "if jax.device_count() == 1:\n",
    "    mesh = Mesh(mesh_utils.create_device_mesh((1, 1)), (\"batch\", \"model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZKdhNo98NgG"
   },
   "source": [
    "We will use the GPT-2 tokenizer from the [Tiktoken](https://github.com/openai/tiktoken) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWbkk1V7-Isg"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XHQ0BQ9-KIj"
   },
   "source": [
    "To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. Earlier, we also imported [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) and [`jax.sharding.NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding):\n",
    "- [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) (using alias `P`) defines how tensors are sharded across the devices in our `Mesh`. Its elements describe how an input dimension is partitioned across mesh dimensions. For example, in `PartitionSpec('x', 'y')` the first dimension of data is sharded across `x` axis of the mesh, and the second one - across the `y` axis.\n",
    "  - We'll use `PartitionSpec` to describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n",
    "- [`NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding) is a (`Mesh`, `PartitionSpec`) pair that describes how to shard a model tensor across our `mesh`.\n",
    "- We combine `Mesh` (the TPU resources) with `PartitionSpec` and create a `NamedSharding`, which instructs how to shard each model tensor across the TPU devices.\n",
    "\n",
    "Additionally, we'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n",
    "- `nnx.with_partitioning` will take two arguments, such as the `initializer` (such as [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)) and `sharding` (e.g. `NamedSharding(Mesh, PartitionSpec)` or `NamedSharding(mesh, P('model')` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0p-IHurrB9i"
   },
   "outputs": [],
   "source": [
    "# Define a triangular mask for causal attention with `jax.numpy.tril` and `jax.numpy.ones`.\n",
    "# [수정 1] 마스크 차원 및 타입 명시\n",
    "# (Batch, Head) 차원으로 브로드캐스팅 되도록 (1, 1, L, L) 형태의 Boolean 마스크 생성\n",
    "def causal_attention_mask(seq_len):\n",
    "    # vLLM/FlashAttention 등과의 호환성을 위해 bool 타입 권장\n",
    "    return jnp.tril(jnp.ones((1, 1, seq_len, seq_len), dtype=bool))\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    \"\"\" A single Transformer block.\n",
    "\n",
    "    Each Transformer block processes input sequences via self-attention and feed-forward networks.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        ff_dim (int): Dimensionality of the feed-forward network.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "        rate (float): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
    "        # [수정 2] 초기화 변경 (Xavier -> Normal(0.02))\n",
    "        # GPT-2는 가중치 초기화 표준편차를 0.02로 설정하는 것이 국룰입니다.\n",
    "        std_init = nnx.initializers.normal(stddev=0.02)\n",
    "        \n",
    "        # Multi-Head Attention (MHA) with `flax.nnx.MultiHeadAttention`.\n",
    "        # Specifies tensor sharding (depending on the mesh configuration)\n",
    "        # where we shard the weights across devices for parallel computation.\n",
    "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
    "                                          in_features=embed_dim,\n",
    "                                          kernel_init=nnx.with_partitioning(std_init, P(None, 'model')),\n",
    "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                          rngs=rngs)\n",
    "        # The first dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout1 = nnx.Dropout(rate=rate)\n",
    "        # First layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-5, # GPT-2 default epsilon\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                         rngs=rngs)\n",
    "        # The first linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
    "                                  out_features=ff_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(std_init, P(None, 'model')),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                  rngs=rngs)\n",
    "        # The second linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
    "                                  out_features=embed_dim,\n",
    "                                  # [Tip] GPT-2는 잔차 연결(Residual) 직전의 레이어 가중치를 1/sqrt(2*layers)로 스케일링하면 더 깊게 쌓을 수 있습니다.\n",
    "                                  kernel_init=nnx.with_partitioning(std_init, P(None, 'model')),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                  rngs=rngs)\n",
    "        # The second dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout2 = nnx.Dropout(rate=rate)\n",
    "        # Second layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-5,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                         rngs=rngs)\n",
    "\n",
    "    # Apply the Transformer block to the input sequence.\n",
    "    def __call__(self, inputs, mask, training: bool = False):\n",
    "        # MHA\n",
    "        norm1 = self.layer_norm1(inputs)\n",
    "        attention_output = self.mha(inputs_q=norm1, mask=mask, decode=False)\n",
    "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
    "        out1 = inputs + attention_output\n",
    "\n",
    "        # FFN\n",
    "        norm2 = self.layer_norm2(out1)\n",
    "        ffn_output = self.linear1(norm2)\n",
    "        # GPT-2는 보통 GELU를 사용하지만, ReLU도 동작은 합니다. vLLM 호환을 위해선 GELU 추천 (여기선 기존 유지)\n",
    "        ffn_output = nnx.relu(ffn_output) \n",
    "        ffn_output = self.linear2(ffn_output)\n",
    "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
    "        \n",
    "        return out1 + ffn_output\n",
    "\n",
    "class TokenAndPositionEmbedding(nnx.Module):\n",
    "    \"\"\" Combines token embeddings (words in an input sentence) with\n",
    "    positional embeddings (the position of each word in a sentence).\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Matimum sequence length.\n",
    "        vocal_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
    "        # Initialize token embeddings (using `flax.nnx.Embed`).\n",
    "        # Each unique word has an embedding vector.\n",
    "        std_init = nnx.initializers.normal(stddev=0.02)\n",
    "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, embedding_init=nnx.with_partitioning(std_init, P('model')), rngs=rngs)\n",
    "        # Initialize positional embeddings (using `flax.nnx.Embed`).\n",
    "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, embedding_init=nnx.with_partitioning(std_init, P('model')), rngs=rngs)\n",
    "\n",
    "    # Takes a token sequence (integers) and returns the combined token and positional embeddings.\n",
    "    def __call__(self, x):\n",
    "        # [주의] 학습 시에는 x가 전체 시퀀스라 이 로직이 맞지만, \n",
    "        # 추론(Generation) 시 한 글자씩 넣으면 positions가 항상 0이 되는 문제가 있습니다.\n",
    "        # vLLM 이용 시에는 KV Cache가 위치 인덱스를 관리하므로 괜찮지만,\n",
    "        # JAX에서 직접 생성 테스트를 하려면 이 부분에 주의해야 합니다.\n",
    "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
    "        position_embedding = self.pos_emb(positions)\n",
    "        token_embedding = self.token_emb(x)\n",
    "        return token_embedding + position_embedding\n",
    "\n",
    "class MiniGPT(nnx.Module):\n",
    "    \"\"\" A miniGPT transformer model, inherits from `flax.nnx.Module`.\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Maximum sequence length.\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        feed_forward_dim (int): Dimensionality of the feed-forward network.\n",
    "        num_transformer_blocks (int): Number of transformer blocks. Each block contains attention and feed-forward networks.\n",
    "        rngs (nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    # Initialize miniGPT model components.\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        # Initiliaze the `TokenAndPositionEmbedding` that combines token and positional embeddings.\n",
    "        print(\"MiniGPT - Initializing Embedding layer\")\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
    "                )\n",
    "        # Create a list of `TransformerBlock` instances.\n",
    "        # Each block processes input sequences using attention and feed-forward networks.\n",
    "        print(\"MiniGPT - Initializing Transformer block\")\n",
    "        self.transformer_blocks = [TransformerBlock(\n",
    "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
    "        ) for _ in range(num_transformer_blocks)]\n",
    "\n",
    "        print(\"MiniGPT - Initializing Final LayerNorm\")\n",
    "        self.ln_f = nnx.LayerNorm(epsilon=1e-5,\n",
    "                                  num_features=embed_dim,\n",
    "                                  scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                  rngs=rngs)\n",
    "\n",
    "        print(\"MiniGPT - Done\")\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        input_shape = inputs.shape\n",
    "        _, seq_len = input_shape\n",
    "        \n",
    "        # 마스크를 여기서 한 번만 생성해서 블록에 전달\n",
    "        mask = causal_attention_mask(seq_len)\n",
    "\n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, mask=mask, training=training)\n",
    "            \n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # [수정 3 적용] Weight Tying (가중치 공유)\n",
    "        # 입력 임베딩 행렬을 가져와서 전치(Transpose)하여 출력 레이어로 사용합니다.\n",
    "        # 이는 GPT 계열 성능 향상 및 반복 생성 방지에 결정적입니다.\n",
    "        embedding_weights = self.embedding_layer.token_emb.embedding.value\n",
    "        # (Batch, Seq, Dim) @ (Vocab, Dim).T -> (Batch, Seq, Vocab)\n",
    "        outputs = x @ embedding_weights.T\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    @nnx.jit\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "        logits = nnx.softmax(logits)\n",
    "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
    "\n",
    "    @nnx.jit\n",
    "    def generate_step(self, padded_tokens, sample_index):\n",
    "        logits = self(padded_tokens)\n",
    "        next_token = self.sample_from(logits[0][sample_index])\n",
    "        return next_token\n",
    "\n",
    "    def generate_text(self, max_tokens, start_tokens):\n",
    "        generated = []\n",
    "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
    "        for i in range(max_tokens):\n",
    "            sample_index = len(start_tokens) + len(generated) - 1\n",
    "\n",
    "            padded_tokens = jnp.array((start_tokens + generated + [0] * (maxlen - len(start_tokens) - len(generated))))[None, :]\n",
    "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
    "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
    "              break\n",
    "            generated.append(next_token)\n",
    "            # decode and print next_token\n",
    "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
    "        return tokenizer.decode(start_tokens + generated)\n",
    "\n",
    "# Creates the miniGPT model with 4 transformer blocks.\n",
    "def create_model(rngs):\n",
    "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=num_transformer_blocks, rngs=rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRhiDsCrMZRp"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "num_transformer_blocks = 4\n",
    "maxlen = 256\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "feed_forward_dim = 256\n",
    "batch_size = 144 * jax.device_count() / 2  # divide by 2 in case of model parallelism\n",
    "if jax.device_count() == 1:\n",
    "    batch_size = 144\n",
    "num_epochs = 1\n",
    "top_k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igX_eoGNMTGR"
   },
   "source": [
    "Set some hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI1ci-HyMspJ"
   },
   "source": [
    "## Loading and preprocessing the data\n",
    "\n",
    "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGUFsn1GMuzh"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextDataset:\n",
    "    data: list\n",
    "    maxlen: int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Use Tiktoken for tokenization\n",
    "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
    "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
    "\n",
    "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "      text = f.read()\n",
    "\n",
    "    stories = text.split('<|endoftext|>')\n",
    "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
    "    df = pd.DataFrame({'text': stories})\n",
    "    data = df['text'].dropna().tolist()\n",
    "    dataset = TextDataset(data, maxlen)\n",
    "\n",
    "    sampler = pygrain.IndexSampler(\n",
    "        len(dataset),\n",
    "        shuffle=False,\n",
    "        seed=42,\n",
    "        shard_options=pygrain.NoSharding(),\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    dl = pygrain.DataLoader(\n",
    "        data_source=dataset,\n",
    "        sampler=sampler,\n",
    "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
    "    )\n",
    "\n",
    "    return dl\n",
    "\n",
    "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKVSD8KSM1um"
   },
   "source": [
    "## Defining the loss function and training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rRuTmABNV4b"
   },
   "outputs": [],
   "source": [
    "# Defines the loss function using `optax.softmax_cross_entropy_with_integer_labels`.\n",
    "def loss_fn(model, batch):\n",
    "    logits = model(batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, logits\n",
    "\n",
    "# Define the training step with the `flax.nnx.jit` transformation decorator.\n",
    "@nnx.jit\n",
    "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
    "    optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5um2vkeUNckm"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Start training. It takes ~50 minutes on Colab.\n",
    "\n",
    "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put` with `NamedSharding`.\n",
    "\n",
    "We are also using the `jax.vmap` transformation to produce the target sequences faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ysl6CsfENeJN",
    "outputId": "5dd06dca-f030-4927-a9b6-35d412da535c"
   },
   "outputs": [],
   "source": [
    "model = create_model(rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n",
    "metrics = nnx.MultiMetric(\n",
    "    loss=nnx.metrics.Average(\"loss\"),\n",
    ")\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "start_prompt = \"Once upon a time\"\n",
    "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
    "print(\"Initial generated text:\")\n",
    "generated_text = model.generate_text(maxlen, start_tokens)\n",
    "\n",
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "}\n",
    "\n",
    "prep_target_batch = jax.vmap(\n",
    "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
    ")\n",
    "\n",
    "step = 0\n",
    "max_step = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for batch in text_dl:\n",
    "        if len(batch) % len(jax.devices()) != 0:\n",
    "            continue  # skip the remaining elements\n",
    "        input_batch = jnp.array(jnp.array(batch).T)\n",
    "        target_batch = prep_target_batch(input_batch)\n",
    "        train_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            metrics,\n",
    "            jax.device_put(\n",
    "                (input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if (step + 1) % 200 == 0:\n",
    "            for metric, value in metrics.compute().items():\n",
    "                metrics_history[f\"train_{metric}\"].append(value)\n",
    "            metrics.reset()\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(\n",
    "                f\"\\n\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\"\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "            print(\"Generated text:\")\n",
    "            generated_text = model.generate_text(maxlen, start_tokens)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        if step == max_step:\n",
    "            break\n",
    "\n",
    "    if step == max_step:\n",
    "        break\n",
    "\n",
    "# Final text generation\n",
    "print(\"Final generated text:\")\n",
    "generated_text = model.generate_text(maxlen, start_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thaLs6TD0lt5"
   },
   "source": [
    "Visualize the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "B6Eg1Cz2y_iP",
    "outputId": "7cafe711-1ae4-4eb9-fd37-e1bde54cbfc5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(metrics_history['train_loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB-ExEt1Zl1C"
   },
   "source": [
    "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 상태(State) 가져오기\n",
    "state = nnx.state(model)\n",
    "\n",
    "# 전체 파라미터(Leaf 노드들)의 크기 합산\n",
    "total_params = sum(x.size for x in jax.tree_util.tree_leaves(state))\n",
    "print(f\"Total Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = nnx.state(model)\n",
    "flat_params = state.flat_state()\n",
    "total_params = 0\n",
    "# items()로 키(경로)와 값(파라미터 텐서)을 순회\n",
    "for path_tuple, param in flat_params:\n",
    "    # 1. 경로 튜플을 문자열로 변환 (예: ('linear1', 'kernel') -> 'linear1.kernel')\n",
    "    path_str = \".\".join(str(p) for p in path_tuple)\n",
    "    \n",
    "    # 2. 파라미터 형상(Shape)과 원소 개수(Size) 추출\n",
    "    # param은 nnx.Variable 또는 jax.Array 형태일 수 있으므로 .value로 실제 값에 접근하거나 직접 속성 사용\n",
    "    tensor = param.value if hasattr(param, 'value') else param\n",
    "    shape = tensor.shape\n",
    "    count = tensor.size\n",
    "    \n",
    "    # 3. 누적 합계 계산\n",
    "    total_params += count\n",
    "    \n",
    "    # 4. 출력\n",
    "    print(f\"{path_str:<60} | {str(shape):<20} | {count:<15,}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"Total Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soPqiR1JNmjf"
   },
   "source": [
    "## Saving the checkpoint\n",
    "\n",
    "Save the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkoFGCgSZ1yz",
    "outputId": "3467b8ba-ce05-42f0-fb89-75922cc91e31"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import orbax.checkpoint as orbax\n",
    "\n",
    "dir = f\"{str(pathlib.Path().resolve())}/save\"\n",
    "state = nnx.state(model)\n",
    "\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "checkpointer.save(dir, args=orbax.args.PyTreeSave(state), force=True)\n",
    "\n",
    "# Make sure the files are there\n",
    "!ls {dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as safetensors format for vLLM load testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import safetensors\n",
    "import numpy as np\n",
    "def save_model_to_vllm_format(model: nnx.Module, save_dir: str = \"./vllm_gpt2_model\"):\n",
    "    \"\"\"\n",
    "    MiniGPT(nnx) 모델을 vLLM 호환 GPT-2 safetensors 포맷으로 저장합니다.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. 모델의 파라미터(State) 추출\n",
    "    # nnx는 객체 지향적이므로 state를 GraphState 형태로 추출합니다.\n",
    "    graph_state = nnx.state(model)\n",
    "    # 딕셔너리 형태로 변환 (경로 탐색을 위해)\n",
    "    params = graph_state.to_pure_dict()\n",
    "\n",
    "    # safetensors에 저장할 딕셔너리\n",
    "    tensors = {}\n",
    "\n",
    "    print(\"Converting weights to Hugging Face GPT-2 format...\")\n",
    "\n",
    "    # --- 1. Embeddings ---\n",
    "    wte = np.array(params['embedding_layer']['token_emb']['embedding'])\n",
    "    tensors[\"transformer.wte.weight\"] = wte\n",
    "    tensors[\"lm_head.weight\"] = wte # Tie weights\n",
    "\n",
    "    wpe = np.array(params['embedding_layer']['pos_emb']['embedding'])\n",
    "    tensors[\"transformer.wpe.weight\"] = wpe\n",
    "\n",
    "    # --- 2. Transformer Blocks ---\n",
    "    blocks = params['transformer_blocks']\n",
    "    num_layers = len(blocks)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        block_key = str(i) if str(i) in blocks else i\n",
    "        block_params = blocks[block_key]\n",
    "        prefix = f\"transformer.h.{i}\"\n",
    "\n",
    "        # LN 1\n",
    "        tensors[f\"{prefix}.ln_1.weight\"] = np.array(block_params['layer_norm1']['scale'])\n",
    "        tensors[f\"{prefix}.ln_1.bias\"] = np.array(block_params['layer_norm1']['bias'])\n",
    "\n",
    "        # --- [핵심 수정] Attention (MHA) Reshape ---\n",
    "        mha = block_params['mha']\n",
    "        \n",
    "        # JAX Shape: (embed_dim, num_heads, head_dim) -> (256, 8, 32)\n",
    "        q_w_raw = np.array(mha['query']['kernel'])\n",
    "        k_w_raw = np.array(mha['key']['kernel'])\n",
    "        v_w_raw = np.array(mha['value']['kernel'])\n",
    "\n",
    "        # Reshape to 2D: (embed_dim, num_heads * head_dim) -> (256, 256)\n",
    "        # 즉, 3차원 텐서의 마지막 두 차원을 하나로 합칩니다.\n",
    "        embed_dim = q_w_raw.shape[0]\n",
    "        hidden_size = q_w_raw.shape[1] * q_w_raw.shape[2] # 8 * 32 = 256\n",
    "        \n",
    "        q_w = q_w_raw.reshape(embed_dim, hidden_size)\n",
    "        k_w = k_w_raw.reshape(embed_dim, hidden_size)\n",
    "        v_w = v_w_raw.reshape(embed_dim, hidden_size)\n",
    "\n",
    "        # Concatenate: (256, 256) * 3 -> (256, 768)\n",
    "        c_attn_w = np.concatenate([q_w, k_w, v_w], axis=-1)\n",
    "        \n",
    "        # Bias Reshape: (num_heads, head_dim) -> (num_heads * head_dim)\n",
    "        q_b = np.array(mha['query']['bias']).reshape(-1)\n",
    "        k_b = np.array(mha['key']['bias']).reshape(-1)\n",
    "        v_b = np.array(mha['value']['bias']).reshape(-1)\n",
    "        c_attn_b = np.concatenate([q_b, k_b, v_b], axis=-1)\n",
    "\n",
    "        tensors[f\"{prefix}.attn.c_attn.weight\"] = c_attn_w\n",
    "        tensors[f\"{prefix}.attn.c_attn.bias\"] = c_attn_b\n",
    "\n",
    "        # --- Attention Output Projection ---\n",
    "        # JAX Shape: (num_heads, head_dim, embed_dim) -> (8, 32, 256)\n",
    "        out_w_raw = np.array(mha['out']['kernel'])\n",
    "        \n",
    "        # Reshape to 2D: (num_heads * head_dim, embed_dim) -> (256, 256)\n",
    "        # HF/vLLM Expects: (embed_dim, embed_dim) for Conv1D weight\n",
    "        # 주의: JAX의 out kernel은 (heads, head_dim, embed) 순서이므로\n",
    "        # 앞의 두 차원을 합쳐야 (256, 256)이 됩니다.\n",
    "        out_w = out_w_raw.reshape(-1, embed_dim)\n",
    "        \n",
    "        tensors[f\"{prefix}.attn.c_proj.weight\"] = out_w\n",
    "        tensors[f\"{prefix}.attn.c_proj.bias\"] = np.array(mha['out']['bias'])\n",
    "\n",
    "        # LN 2\n",
    "        tensors[f\"{prefix}.ln_2.weight\"] = np.array(block_params['layer_norm2']['scale'])\n",
    "        tensors[f\"{prefix}.ln_2.bias\"] = np.array(block_params['layer_norm2']['bias'])\n",
    "\n",
    "        # MLP (JAX Linear는 이미 2D (in, out) 이므로 reshape 불필요)\n",
    "        tensors[f\"{prefix}.mlp.c_fc.weight\"] = np.array(block_params['linear1']['kernel'])\n",
    "        tensors[f\"{prefix}.mlp.c_fc.bias\"] = np.array(block_params['linear1']['bias'])\n",
    "        tensors[f\"{prefix}.mlp.c_proj.weight\"] = np.array(block_params['linear2']['kernel'])\n",
    "        tensors[f\"{prefix}.mlp.c_proj.bias\"] = np.array(block_params['linear2']['bias'])\n",
    "\n",
    "    # --- 3. Final Layer Norm ---\n",
    "    tensors[\"transformer.ln_f.weight\"] = np.array(params['ln_f']['scale'])\n",
    "    tensors[\"transformer.ln_f.bias\"] = np.array(params['ln_f']['bias'])\n",
    "\n",
    "    # 4. Safetensors 파일 저장\n",
    "    save_path = os.path.join(save_dir, \"model.safetensors\")\n",
    "    safetensors.flax.save_file(tensors, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    # 5. Config.json 생성 (vLLM 로드를 위해 필수)\n",
    "    # 모델 하이퍼파라미터에 맞춰 수정하세요.\n",
    "    config = {\n",
    "        \"architectures\": [\"GPT2LMHeadModel\"],\n",
    "        \"model_type\": \"gpt2\",\n",
    "        \"vocab_size\": model.embedding_layer.token_emb.num_embeddings,\n",
    "        \"n_positions\": model.embedding_layer.pos_emb.num_embeddings,\n",
    "        \"n_embd\": wte.shape[1],\n",
    "        \"n_layer\": num_layers,\n",
    "        \"n_head\": num_heads,  # 모델 생성시 사용한 num_heads 값 (코드 상 변수 참조 필요)\n",
    "        \"n_inner\": block_params['linear1']['kernel'].shape[1], # feed_forward_dim\n",
    "        \"activation_function\": \"relu\", # 학습 코드에서 relu 사용함. (보통 gpt2는 gelu_new)\n",
    "        \"resid_pdrop\": 0.1,\n",
    "        \"embd_pdrop\": 0.1,\n",
    "        \"attn_pdrop\": 0.1,\n",
    "        \"layer_norm_epsilon\": 1e-5,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"bos_token_id\": vocab_size, # Tokenizer에 맞게 수정\n",
    "        \"eos_token_id\": vocab_size, # Tokenizer에 맞게 수정\n",
    "        \"tie_word_embeddings\": True\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"Config saved to {os.path.join(save_dir, 'config.json')}\")\n",
    "\n",
    "save_model_to_vllm_format(model, save_dir=\"./MiniGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage cp -r MiniGPT gs://$(gcloud config get-value project)/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "# 기존 TransformerBlock, TokenAndPositionEmbedding, causal_attention_mask는 그대로 사용합니다.\n",
    "# (위에 정의된 코드를 재사용한다고 가정)\n",
    "\n",
    "class MiniGPTEmbedding(nnx.Module):\n",
    "    \"\"\"\n",
    "    MiniGPT based Text Embedding Model.\n",
    "    Generates a fixed-size vector representation for input text.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, \n",
    "                 feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        \n",
    "        # 1. 임베딩 레이어 (동일)\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "            maxlen, vocab_size, embed_dim, rngs=rngs\n",
    "        )\n",
    "        \n",
    "        # 2. 트랜스포머 블록 스택 (동일)\n",
    "        self.transformer_blocks = [TransformerBlock(\n",
    "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
    "        ) for _ in range(num_transformer_blocks)]\n",
    "        \n",
    "        # [변경점 1] Output Layer(Linear to vocab_size)를 제거했습니다.\n",
    "        # 대신 임베딩 차원을 유지하거나, 특정 차원으로 줄이는 Projection Layer를 둘 수도 있습니다.\n",
    "        # 여기서는 Raw Hidden State를 그대로 사용합니다.\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        # 1. Embed inputs\n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "        # 2. Pass through Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "            \n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        # [변경점 2] Logits을 반환하지 않고, 마지막 Hidden State 전체를 반환합니다.\n",
    "        return x\n",
    "\n",
    "    @nnx.jit\n",
    "    def encode(self, inputs, attention_mask=None):\n",
    "        \"\"\"\n",
    "        입력 텍스트를 하나의 벡터로 변환(Pooling)하여 반환합니다.\n",
    "        여기서는 가장 널리 쓰이는 'Mean Pooling'을 구현합니다.\n",
    "        \"\"\"\n",
    "        # (Batch, Seq, Dim) 형태의 Hidden State 획득\n",
    "        hidden_states = self(inputs, training=False)\n",
    "        \n",
    "        # Mask가 없으면 패딩(0)이 아닌 부분만 1로 간주\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (inputs != 0).astype(jnp.float32)\n",
    "            \n",
    "        # 차원 확장: (Batch, Seq) -> (Batch, Seq, 1)\n",
    "        mask_expanded = attention_mask[:, :, None]\n",
    "        \n",
    "        # --- Mean Pooling 전략 ---\n",
    "        # 패딩이 아닌 토큰들의 벡터만 합산\n",
    "        sum_embeddings = jnp.sum(hidden_states * mask_expanded, axis=1)\n",
    "        # 패딩이 아닌 토큰의 개수 합산 (0으로 나누기 방지용 clamp)\n",
    "        sum_mask = jnp.clip(mask_expanded.sum(axis=1), a_min=1e-9)\n",
    "        \n",
    "        # 평균 계산\n",
    "        sentence_embedding = sum_embeddings / sum_mask\n",
    "        \n",
    "        # (Optional) L2 정규화: 코사인 유사도 계산을 쉽게 하기 위함\n",
    "        norm = jnp.linalg.norm(sentence_embedding, axis=1, keepdims=True)\n",
    "        return sentence_embedding / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(42)\n",
    "embedding_model = MiniGPTEmbedding(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks, rngs)\n",
    "\n",
    "# 3. 더미 데이터 (Batch size = 2, 문장 길이 = 32)\n",
    "sentence1 = tokenizer.encode(\"Today is Christmas\")\n",
    "sentence2 = tokenizer.encode(\"25 DEC is here\")\n",
    "input_ids = jnp.array([\n",
    "    sentence1 + [0] * (maxlen - len(sentence1)),\n",
    "    sentence2 + [0] * (maxlen - len(sentence2))\n",
    "])\n",
    "\n",
    "# 4. 임베딩 벡터 추출 (Encode)\n",
    "embeddings = embedding_model.encode(input_ids)\n",
    "\n",
    "print(f\"Embedding Shape: {embeddings.shape}\") \n",
    "# 출력: (2, 256) -> (배치 크기, 임베딩 차원)\n",
    "\n",
    "# 5. 코사인 유사도 계산 (벡터 내적)\n",
    "# 정규화가 되어 있으므로 내적(dot product)이 곧 코사인 유사도입니다.\n",
    "vec_a = embeddings[0]\n",
    "vec_b = embeddings[1]\n",
    "similarity = jnp.dot(vec_a, vec_b)\n",
    "\n",
    "print(f\"Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813cbf2",
   "metadata": {},
   "source": [
    "## Profiling for hyperparameter tuning\n",
    "\n",
    "**Note:** this section assume multiple TPU cores. Free-tier Colab TPU v5e-1 cannot run here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5fc4d",
   "metadata": {},
   "source": [
    "Load the tensorboard colab extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6131f",
   "metadata": {},
   "source": [
    "As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace/\"\n",
    "\n",
    "def loop_step(batch, step):\n",
    "    input_batch = jnp.array(jnp.array(batch).T)\n",
    "    target_batch = prep_target_batch(input_batch)\n",
    "    train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
    "\n",
    "def generate_trace():\n",
    "    tracing_steps = 30\n",
    "    warmup_steps = 5\n",
    "    for current_step in range(warmup_steps + tracing_steps):\n",
    "        if current_step == warmup_steps:\n",
    "            jax.profiler.start_trace(trace_dir)\n",
    "        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n",
    "            batch = next(text_dl)\n",
    "            loop_step(batch, current_step)\n",
    "\n",
    "    jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70f5b7",
   "metadata": {},
   "source": [
    "Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9452a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-batch-comparison/\"\n",
    "\n",
    "batch_size = 64\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()\n",
    "\n",
    "batch_size = 256\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea379965",
   "metadata": {},
   "source": [
    "Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 256`.\n",
    "\n",
    "The key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n",
    "\n",
    "In general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from 64 -> 256 achieves both of those. FLOPS increases from 16% to 27%. Average Step Time increase from 100ms to 260ms, however we increased our batch size by 300%. This means we move from 1.5ms per training example to 1.02ms per training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657967a5",
   "metadata": {},
   "source": [
    "Next, we can explore alternative parallelism methods. In cell #4, we used 4-way data parallel and 2-way tensor parallel. 8-way data parallel is another popular way. Let's compare results between them. To switch to 8-way data parallel, we'll replace the `Mesh` definition with:\n",
    "\n",
    "`mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))`\n",
    "\n",
    "JAX will automatically figure out how to shard the model and data to use the new partition strategy and nothing else need to be done. Re-connect the TPU runtime and run it again to see how it runs.\n",
    "\n",
    "How simple and powerful is this! And that's the beauty of JAX automatic parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-parallelism-comparison/\"\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "generate_trace()\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96e72b",
   "metadata": {},
   "source": [
    "Once again we'll run tensorboard.\n",
    "\n",
    "Looking at the results, we see that the step times are nearly the same, however the FLOPS Utilization is at 13% for 8-way data parallelism compared to 27% or 4-way data parallelism.\n",
    "\n",
    "By looking at the Trace Viewer tool and looking under each TPU's ops, we can see that the TPUs spend a large amount of time idle while waiting for the host, as well as spending a good amount of time in `reduce_sum` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca486e",
   "metadata": {},
   "source": [
    "By changing hyperparameters and comparing profiles, we're able to gain significant insights into our bottlenecks and limitations. These are just two examples of hyperparameters to tune, but plenty more of them will have significant effects on training speed and resource utilization."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
