{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a miniGPT language model with JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.kaggle.com/static/images/logos/kaggle-logo-transparent-300.png\" height=\"32\" width=\"70\"/>Run in Kaggle</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIOXoY1xgiww"
   },
   "source": [
    "This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for [Single-Program Multi-Data](https://en.wikipedia.org/wiki/Single_program,_multiple_data)). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
    "\n",
    "Here, you will learn how to:\n",
    "\n",
    "- Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "- Load and preprocess the dataset\n",
    "- Create the loss and training step functions\n",
    "- Train the model on TPUs on Kaggle or Google Colab\n",
    "- Profile for hyperparameter tuning\n",
    "\n",
    "If you are new to JAX for AI, check out the [introductory tutorial](https://jax-ai-stack.readthedocs.io/en/latest/neural_net_basics.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTmz5Cbco7n_"
   },
   "source": [
    "## Setup\n",
    "\n",
    "JAX installation is covered in [this guide](https://jax.readthedocs.io/en/latest/installation.html) on the JAX documentation site. We will use [Tiktoken](https://github.com/openai/tiktoken) for tokenization and [Grain](https://google-grain.readthedocs.io/en/latest/index.html) for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zMsOIc7ouCO",
    "outputId": "037d56a9-b18f-4504-f80a-3a4fa2945068"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq \"jax[tpu]\" pandas tiktoken jax-ai-stack[grain] matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHzJ_bokoovZ"
   },
   "source": [
    "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUjQsgQEmI1N",
    "outputId": "e6eff24e-5578-4277-a0f9-24e27bd91ee0"
   },
   "outputs": [],
   "source": [
    "#Need fix to use GCS bucket \n",
    "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rcji_799n4eA"
   },
   "source": [
    "**Note:** If you are using [Kaggle](https://www.kaggle.com/), select the free TPU v5e-8 as the hardware accelerator. If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v5e-1 as the hardware accelerator. You may also use Google Cloud TPUs.\n",
    "\n",
    "Check the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS9sQEY3n0mB",
    "outputId": "9ffcf3a6-20ef-4f80-b006-f5d3c5644a15"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKE2uUafLobI"
   },
   "source": [
    "Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKYFNOhdLq98"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding # For data and model parallelism (explained in more detail later)\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import grain.python as pygrain\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPyt7MV6prz1"
   },
   "source": [
    "## Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "\n",
    "### Leveraging JAX's data and tensor parallelism\n",
    "\n",
    "One of the most powerful features of JAX is [device parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for SPMD.\n",
    "\n",
    "- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n",
    "- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n",
    "- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n",
    "\n",
    "In this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup, which is aligned with Kaggle TPU v5e-8 or newer GCP TPUs chips.\n",
    "\n",
    "Note that as of October 2025, free-tier Colab only offers TPU v5e-1, which can no longer support SPMD.\n",
    "\n",
    "### jax.sharding.Mesh\n",
    "\n",
    "Earlier, we imported [`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) - is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n",
    "\n",
    "Our `Mesh` will have two arguments:\n",
    "- `devices`: This will take the value of [`jax.experimental.mesh_utils((4, 2))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n",
    "- `axis_names`, where:\n",
    "  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n",
    "  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor parallism\n",
    "\n",
    "This matches the structure in the Kaggle TPU v5e setup.\n",
    "\n",
    "Let's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuMlCK3Q8WJD"
   },
   "outputs": [],
   "source": [
    "# Create a `Mesh` object representing TPU device arrangement.\n",
    "# For example, for Kaggle TPU v5e-8:\n",
    "if jax.device_count() == 8:\n",
    "    mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "\n",
    "    ### Alternatively, we could use the 8-way data parallelism with only one line of code change.\n",
    "    ### JAX enables quick experimentation with different partitioning strategies\n",
    "    ### like this. We will come back to this point at the end of this tutorial.\n",
    "    # mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
    "\n",
    "### For free-tier Colab TPU, which only has a single TPU core\n",
    "if jax.device_count() == 1:\n",
    "    mesh = Mesh(mesh_utils.create_device_mesh((1, 1)), (\"batch\", \"model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZKdhNo98NgG"
   },
   "source": [
    "We will use the GPT-2 tokenizer from the [Tiktoken](https://github.com/openai/tiktoken) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWbkk1V7-Isg"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XHQ0BQ9-KIj"
   },
   "source": [
    "To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. Earlier, we also imported [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) and [`jax.sharding.NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding):\n",
    "- [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) (using alias `P`) defines how tensors are sharded across the devices in our `Mesh`. Its elements describe how an input dimension is partitioned across mesh dimensions. For example, in `PartitionSpec('x', 'y')` the first dimension of data is sharded across `x` axis of the mesh, and the second one - across the `y` axis.\n",
    "  - We'll use `PartitionSpec` to describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n",
    "- [`NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding) is a (`Mesh`, `PartitionSpec`) pair that describes how to shard a model tensor across our `mesh`.\n",
    "- We combine `Mesh` (the TPU resources) with `PartitionSpec` and create a `NamedSharding`, which instructs how to shard each model tensor across the TPU devices.\n",
    "\n",
    "Additionally, we'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n",
    "- `nnx.with_partitioning` will take two arguments, such as the `initializer` (such as [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)) and `sharding` (e.g. `NamedSharding(Mesh, PartitionSpec)` or `NamedSharding(mesh, P('model')` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0p-IHurrB9i"
   },
   "outputs": [],
   "source": [
    "# Define a triangular mask for causal attention with `jax.numpy.tril` and `jax.numpy.ones`.\n",
    "def causal_attention_mask(seq_len):\n",
    "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    \"\"\" A single Transformer block.\n",
    "\n",
    "    Each Transformer block processes input sequences via self-attention and feed-forward networks.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        ff_dim (int): Dimensionality of the feed-forward network.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "        rate (float): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
    "        # Multi-Head Attention (MHA) with `flax.nnx.MultiHeadAttention`.\n",
    "        # Specifies tensor sharding (depending on the mesh configuration)\n",
    "        # where we shard the weights across devices for parallel computation.\n",
    "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
    "                                          in_features=embed_dim,\n",
    "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n",
    "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                          rngs=rngs)\n",
    "        # The first dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout1 = nnx.Dropout(rate=rate)\n",
    "        # First layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                         rngs=rngs)\n",
    "        # The first linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
    "                                  out_features=ff_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                  rngs=rngs)\n",
    "        # The second linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
    "                                  out_features=embed_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                  rngs=rngs)\n",
    "        # The second dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout2 = nnx.Dropout(rate=rate)\n",
    "        # Second layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                         rngs=rngs)\n",
    "\n",
    "    # Apply the Transformer block to the input sequence.\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        input_shape = inputs.shape\n",
    "        _, seq_len, _ = input_shape\n",
    "\n",
    "        # Instantiate the causal attention mask.\n",
    "        mask = causal_attention_mask(seq_len)\n",
    "\n",
    "        # ---- [수정됨] Attention Pre-LN ----\n",
    "        # 1. 먼저 정규화를 합니다.\n",
    "        norm1 = self.layer_norm1(inputs)\n",
    "        # 2. 정규화된 값으로 어텐션을 구합니다.\n",
    "        # Apply Multi-Head Attention with the causal attention mask.\n",
    "        attention_output = self.mha(\n",
    "            inputs_q=norm1,\n",
    "            mask=mask,\n",
    "            decode=False\n",
    "        )\n",
    "        # Apply the first dropout.\n",
    "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
    "        # 3. 원본 입력값에 더합니다.\n",
    "        out1 = inputs + attention_output\n",
    "\n",
    "        # ---- [수정됨] FFN Pre-LN ----\n",
    "        # 1. 먼저 정규화를 합니다.\n",
    "        norm2 = self.layer_norm2(out1)\n",
    "        # 2. 정규화된 값으로 FFN을 구합니다.\n",
    "        ffn_output = self.linear1(norm2)  # out1 대신 norm2 사용\n",
    "        # Apply the ReLU activation with `flax.nnx.relu`.\n",
    "        ffn_output = nnx.relu(ffn_output)\n",
    "        # Apply the second linear transformation.\n",
    "        ffn_output = self.linear2(ffn_output)\n",
    "        # Apply the second dropout.\n",
    "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
    "        # Apply the second layer normalization and return the output of the Transformer block.\n",
    "        # 3. 원본(out1)에 더합니다.\n",
    "        return out1 + ffn_output\n",
    "\n",
    "class TokenAndPositionEmbedding(nnx.Module):\n",
    "    \"\"\" Combines token embeddings (words in an input sentence) with\n",
    "    positional embeddings (the position of each word in a sentence).\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Matimum sequence length.\n",
    "        vocal_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
    "        # Initialize token embeddings (using `flax.nnx.Embed`).\n",
    "        # Each unique word has an embedding vector.\n",
    "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
    "        # Initialize positional embeddings (using `flax.nnx.Embed`).\n",
    "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
    "\n",
    "    # Takes a token sequence (integers) and returns the combined token and positional embeddings.\n",
    "    def __call__(self, x):\n",
    "        # Generate a sequence of positions for the input tokens.\n",
    "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
    "        # Look up the positional embeddings for each position in the input sequence.\n",
    "        position_embedding = self.pos_emb(positions)\n",
    "        # Look up the token embeddings for each token in the input sequence.\n",
    "        token_embedding = self.token_emb(x)\n",
    "        # Combine token and positional embeddings.\n",
    "        return token_embedding + position_embedding\n",
    "\n",
    "class MiniGPT(nnx.Module):\n",
    "    \"\"\" A miniGPT transformer model, inherits from `flax.nnx.Module`.\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Maximum sequence length.\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        feed_forward_dim (int): Dimensionality of the feed-forward network.\n",
    "        num_transformer_blocks (int): Number of transformer blocks. Each block contains attention and feed-forward networks.\n",
    "        rngs (nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    # Initialize miniGPT model components.\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        # Initiliaze the `TokenAndPositionEmbedding` that combines token and positional embeddings.\n",
    "        print(\"MiniGPT - Initializing Embedding layer\")\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
    "                )\n",
    "        # Create a list of `TransformerBlock` instances.\n",
    "        # Each block processes input sequences using attention and feed-forward networks.\n",
    "        print(\"MiniGPT - Initializing Transformer block\")\n",
    "        self.transformer_blocks = [TransformerBlock(\n",
    "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
    "        ) for _ in range(num_transformer_blocks)]\n",
    "\n",
    "        print(\"MiniGPT - Initializing Final LayerNorm\")\n",
    "        self.ln_f = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                  num_features=embed_dim,\n",
    "                                  scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                  rngs=rngs)\n",
    "\n",
    "        # Initialize the output `flax.nnx.Linear` layer producing logits over the vocabulary for next-token prediction.\n",
    "        print(\"MiniGPT - Initializing Output layer\")\n",
    "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
    "                                       out_features=vocab_size,\n",
    "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P('model')),\n",
    "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                       rngs=rngs)\n",
    "        print(\"MiniGPT - Done\")\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        # Pass the input tokens through the `embedding_layer` to get token embeddings.\n",
    "        # Apply each transformer block sequentially to the embedded input, use the `training` flag for the behavior of `flax.nnx.Dropout`.\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        # [추가됨] 출력층 직전에 Final LayerNorm 통과\n",
    "        x = self.ln_f(x)\n",
    "        # Pass the output of the transformer blocks through the output layer,\n",
    "        # and obtain logits for each token in the vocabulary (for next token prediction).\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    @nnx.jit\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "        logits = nnx.softmax(logits)\n",
    "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
    "\n",
    "    @nnx.jit\n",
    "    def generate_step(self, padded_tokens, sample_index):\n",
    "        logits = self(padded_tokens)\n",
    "        next_token = self.sample_from(logits[0][sample_index])\n",
    "        return next_token\n",
    "\n",
    "    def generate_text(self, max_tokens, start_tokens):\n",
    "        generated = []\n",
    "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
    "        for i in range(max_tokens):\n",
    "            sample_index = len(start_tokens) + len(generated) - 1\n",
    "\n",
    "            padded_tokens = jnp.array((start_tokens + generated + [0] * (maxlen - len(start_tokens) - len(generated))))[None, :]\n",
    "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
    "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
    "              break\n",
    "            generated.append(next_token)\n",
    "            # decode and print next_token\n",
    "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
    "        return tokenizer.decode(start_tokens + generated)\n",
    "\n",
    "# Creates the miniGPT model with 4 transformer blocks.\n",
    "def create_model(rngs):\n",
    "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=num_transformer_blocks, rngs=rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRhiDsCrMZRp"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "num_transformer_blocks = 4\n",
    "maxlen = 256\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "feed_forward_dim = 256\n",
    "batch_size = 144 * jax.device_count() / 2  # divide by 2 in case of model parallelism\n",
    "if jax.device_count() == 1:\n",
    "    batch_size = 144\n",
    "num_epochs = 1\n",
    "top_k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igX_eoGNMTGR"
   },
   "source": [
    "Set some hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI1ci-HyMspJ"
   },
   "source": [
    "## Loading and preprocessing the data\n",
    "\n",
    "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGUFsn1GMuzh"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextDataset:\n",
    "    data: list\n",
    "    maxlen: int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Use Tiktoken for tokenization\n",
    "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
    "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
    "\n",
    "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "      text = f.read()\n",
    "\n",
    "    stories = text.split('<|endoftext|>')\n",
    "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
    "    df = pd.DataFrame({'text': stories})\n",
    "    data = df['text'].dropna().tolist()\n",
    "    dataset = TextDataset(data, maxlen)\n",
    "\n",
    "    sampler = pygrain.IndexSampler(\n",
    "        len(dataset),\n",
    "        shuffle=False,\n",
    "        seed=42,\n",
    "        shard_options=pygrain.NoSharding(),\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    dl = pygrain.DataLoader(\n",
    "        data_source=dataset,\n",
    "        sampler=sampler,\n",
    "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
    "    )\n",
    "\n",
    "    return dl\n",
    "\n",
    "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKVSD8KSM1um"
   },
   "source": [
    "## Defining the loss function and training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rRuTmABNV4b"
   },
   "outputs": [],
   "source": [
    "# Defines the loss function using `optax.softmax_cross_entropy_with_integer_labels`.\n",
    "def loss_fn(model, batch):\n",
    "    logits = model(batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, logits\n",
    "\n",
    "# Define the training step with the `flax.nnx.jit` transformation decorator.\n",
    "@nnx.jit\n",
    "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
    "    optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5um2vkeUNckm"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Start training. It takes ~50 minutes on Colab.\n",
    "\n",
    "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put` with `NamedSharding`.\n",
    "\n",
    "We are also using the `jax.vmap` transformation to produce the target sequences faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ysl6CsfENeJN",
    "outputId": "5dd06dca-f030-4927-a9b6-35d412da535c"
   },
   "outputs": [],
   "source": [
    "model = create_model(rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n",
    "metrics = nnx.MultiMetric(\n",
    "    loss=nnx.metrics.Average(\"loss\"),\n",
    ")\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "start_prompt = \"Once upon a time\"\n",
    "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
    "print(\"Initial generated text:\")\n",
    "generated_text = model.generate_text(maxlen, start_tokens)\n",
    "\n",
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "}\n",
    "\n",
    "prep_target_batch = jax.vmap(\n",
    "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
    ")\n",
    "\n",
    "step = 0\n",
    "max_step = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for batch in text_dl:\n",
    "        if len(batch) % len(jax.devices()) != 0:\n",
    "            continue  # skip the remaining elements\n",
    "        input_batch = jnp.array(jnp.array(batch).T)\n",
    "        target_batch = prep_target_batch(input_batch)\n",
    "        train_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            metrics,\n",
    "            jax.device_put(\n",
    "                (input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if (step + 1) % 200 == 0:\n",
    "            for metric, value in metrics.compute().items():\n",
    "                metrics_history[f\"train_{metric}\"].append(value)\n",
    "            metrics.reset()\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(\n",
    "                f\"\\n\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\"\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "            print(\"Generated text:\")\n",
    "            generated_text = model.generate_text(maxlen, start_tokens)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        if step == max_step:\n",
    "            break\n",
    "\n",
    "    if step == max_step:\n",
    "        break\n",
    "\n",
    "# Final text generation\n",
    "print(\"Final generated text:\")\n",
    "generated_text = model.generate_text(maxlen, start_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thaLs6TD0lt5"
   },
   "source": [
    "Visualize the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "B6Eg1Cz2y_iP",
    "outputId": "7cafe711-1ae4-4eb9-fd37-e1bde54cbfc5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(metrics_history['train_loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB-ExEt1Zl1C"
   },
   "source": [
    "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 상태(State) 가져오기\n",
    "state = nnx.state(model)\n",
    "\n",
    "# 전체 파라미터(Leaf 노드들)의 크기 합산\n",
    "total_params = sum(x.size for x in jax.tree_util.tree_leaves(state))\n",
    "print(f\"Total Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = nnx.state(model)\n",
    "flat_params = state.flat_state()\n",
    "total_params = 0\n",
    "# items()로 키(경로)와 값(파라미터 텐서)을 순회\n",
    "for path_tuple, param in flat_params:\n",
    "    # 1. 경로 튜플을 문자열로 변환 (예: ('linear1', 'kernel') -> 'linear1.kernel')\n",
    "    path_str = \".\".join(str(p) for p in path_tuple)\n",
    "    \n",
    "    # 2. 파라미터 형상(Shape)과 원소 개수(Size) 추출\n",
    "    # param은 nnx.Variable 또는 jax.Array 형태일 수 있으므로 .value로 실제 값에 접근하거나 직접 속성 사용\n",
    "    tensor = param.value if hasattr(param, 'value') else param\n",
    "    shape = tensor.shape\n",
    "    count = tensor.size\n",
    "    \n",
    "    # 3. 누적 합계 계산\n",
    "    total_params += count\n",
    "    \n",
    "    # 4. 출력\n",
    "    print(f\"{path_str:<60} | {str(shape):<20} | {count:<15,}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"Total Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soPqiR1JNmjf"
   },
   "source": [
    "## Saving the checkpoint\n",
    "\n",
    "Save the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkoFGCgSZ1yz",
    "outputId": "3467b8ba-ce05-42f0-fb89-75922cc91e31"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import orbax.checkpoint as orbax\n",
    "\n",
    "dir = f\"{str(pathlib.Path().resolve())}/save\"\n",
    "state = nnx.state(model)\n",
    "\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "checkpointer.save(dir, args=orbax.args.PyTreeSave(state), force=True)\n",
    "\n",
    "# Make sure the files are there\n",
    "!ls {dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as safetensors format for vLLM load testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import safetensors.flax\n",
    "def convert_minigpt_to_hf_gpt2(model_instance, output_path=\"model.safetensors\"):\n",
    "    state_dict = {}\n",
    "    \n",
    "    # NNX 모델에서 가중치(State) 접근\n",
    "    # model_instance는 학습이 완료된 MiniGPT 객체여야 합니다.\n",
    "    \n",
    "    # --- 1. Embeddings ---\n",
    "    # Flax: embedding_layer.token_emb -> HF: wte\n",
    "    state_dict[\"transformer.wte.weight\"] = torch.from_numpy(\n",
    "        np.array(model_instance.embedding_layer.token_emb.embedding.value)\n",
    "    )\n",
    "    # Flax: embedding_layer.pos_emb -> HF: wpe\n",
    "    state_dict[\"transformer.wpe.weight\"] = torch.from_numpy(\n",
    "        np.array(model_instance.embedding_layer.pos_emb.embedding.value)\n",
    "    )\n",
    "\n",
    "    # --- 2. Transformer Blocks ---\n",
    "    for i, block in enumerate(model_instance.transformer_blocks):\n",
    "        prefix = f\"transformer.h.{i}\"\n",
    "        \n",
    "        # LayerNorm 1\n",
    "        state_dict[f\"{prefix}.ln_1.weight\"] = torch.from_numpy(np.array(block.layer_norm1.scale.value))\n",
    "        state_dict[f\"{prefix}.ln_1.bias\"] = torch.from_numpy(np.array(block.layer_norm1.bias.value))\n",
    "        \n",
    "        # Attention (MHA)\n",
    "        # Flax MHA는 query, key, value가 분리되어 있으나 HF GPT2는 Conv1D로 합쳐져 있음 (Q,K,V 순서)\n",
    "        # 또한, GPT2 Conv1D 가중치는 (hidden_dim, 3*hidden_dim) 형태임.\n",
    "        q_w = np.array(block.mha.query.kernel.value) # (dim, heads, head_dim) or (dim, dim)\n",
    "        k_w = np.array(block.mha.key.kernel.value)\n",
    "        v_w = np.array(block.mha.value.kernel.value)\n",
    "        \n",
    "        # 차원 확인 및 병합: (Hidden, 3 * Hidden)\n",
    "        # NNX MHA 구현에 따라 shape이 (features, num_heads, head_features)일 수 있음. \n",
    "        # 이를 (features, full_dim)으로 reshape 필요할 수 있음.\n",
    "        # 여기서는 튜토리얼의 일반적 linear projection이라 가정하고 dim 축으로 concat\n",
    "        q_w = q_w.reshape(q_w.shape[0], -1)\n",
    "        k_w = k_w.reshape(k_w.shape[0], -1)\n",
    "        v_w = v_w.reshape(v_w.shape[0], -1)\n",
    "        \n",
    "        c_attn_weight = np.concatenate([q_w, k_w, v_w], axis=-1)\n",
    "        state_dict[f\"{prefix}.attn.c_attn.weight\"] = torch.from_numpy(c_attn_weight)\n",
    "        \n",
    "        # Attention Bias\n",
    "        q_b = np.array(block.mha.query.bias.value).reshape(-1)\n",
    "        k_b = np.array(block.mha.key.bias.value).reshape(-1)\n",
    "        v_b = np.array(block.mha.value.bias.value).reshape(-1)\n",
    "        c_attn_bias = np.concatenate([q_b, k_b, v_b], axis=-1)\n",
    "        state_dict[f\"{prefix}.attn.c_attn.bias\"] = torch.from_numpy(c_attn_bias)\n",
    "\n",
    "        # Attention Output Projection\n",
    "        # Flax: mha.out -> HF: attn.c_proj\n",
    "        #out_w = np.array(block.mha.out.kernel.value)\n",
    "        #out_w = out_w.reshape(out_w.shape[0], -1) # flatten heads if needed\n",
    "        # Flax MHA Output Kernel Shape: (num_heads, head_dim, embed_dim) -> (8, 32, 256)\n",
    "        out_w = np.array(block.mha.out.kernel.value)\n",
    "        \n",
    "        # 1. (num_heads, head_dim, embed_dim) -> (num_heads * head_dim, embed_dim)\n",
    "        # 즉, (8 * 32, 256) -> (256, 256)으로 변환\n",
    "        out_w = out_w.reshape(-1, out_w.shape[-1])\n",
    "        state_dict[f\"{prefix}.attn.c_proj.weight\"] = torch.from_numpy(out_w)\n",
    "        state_dict[f\"{prefix}.attn.c_proj.bias\"] = torch.from_numpy(np.array(block.mha.out.bias.value).reshape(-1))\n",
    "\n",
    "        # LayerNorm 2\n",
    "        state_dict[f\"{prefix}.ln_2.weight\"] = torch.from_numpy(np.array(block.layer_norm2.scale.value))\n",
    "        state_dict[f\"{prefix}.ln_2.bias\"] = torch.from_numpy(np.array(block.layer_norm2.bias.value))\n",
    "\n",
    "        # MLP (Feed Forward)\n",
    "        # Flax: linear1 -> HF: mlp.c_fc\n",
    "        state_dict[f\"{prefix}.mlp.c_fc.weight\"] = torch.from_numpy(np.array(block.linear1.kernel.value))\n",
    "        state_dict[f\"{prefix}.mlp.c_fc.bias\"] = torch.from_numpy(np.array(block.linear1.bias.value))\n",
    "        \n",
    "        # Flax: linear2 -> HF: mlp.c_proj\n",
    "        state_dict[f\"{prefix}.mlp.c_proj.weight\"] = torch.from_numpy(np.array(block.linear2.kernel.value))\n",
    "        state_dict[f\"{prefix}.mlp.c_proj.bias\"] = torch.from_numpy(np.array(block.linear2.bias.value))\n",
    "\n",
    "    # --- 3. Final LayerNorm (중요) ---\n",
    "    # 튜토리얼의 MiniGPT에는 Transformer Block 이후 Final LayerNorm이 없습니다.\n",
    "    # 하지만 GPT-2 규격(vLLM)은 transformer.ln_f를 기대합니다.\n",
    "    # 에러 방지를 위해 Identity LayerNorm(weight=1, bias=0)을 임의로 추가합니다.\n",
    "    hidden_dim = state_dict[\"transformer.wte.weight\"].shape[1]\n",
    "    state_dict[\"transformer.ln_f.weight\"] = torch.ones(hidden_dim)\n",
    "    state_dict[\"transformer.ln_f.bias\"] = torch.zeros(hidden_dim)\n",
    "\n",
    "    # --- 4. LM Head ---\n",
    "    # Flax: output_layer (Features, Vocab) -> HF: lm_head (Vocab, Features)\n",
    "    # PyTorch Linear 레이어는 (Out, In)이므로 Transpose가 필요합니다.\n",
    "    # (주의: GPT-2 Conv1D 레이어들은 (In, Out)을 쓰지만, lm_head는 Linear라서 (Out, In)입니다)\n",
    "    lm_head_w = np.array(model_instance.output_layer.kernel.value)\n",
    "    state_dict[\"lm_head.weight\"] = torch.from_numpy(lm_head_w.T).contiguous()    \n",
    "\n",
    "    # 저장\n",
    "    for key, value in state_dict.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    safetensors.flax.save_file(state_dict, output_path)\n",
    "    print(f\"Saved model to {output_path}\")\n",
    "\n",
    "# 사용 예시\n",
    "convert_minigpt_to_hf_gpt2(model, \"MiniGPT/minigpt.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "# 기존 TransformerBlock, TokenAndPositionEmbedding, causal_attention_mask는 그대로 사용합니다.\n",
    "# (위에 정의된 코드를 재사용한다고 가정)\n",
    "\n",
    "class MiniGPTEmbedding(nnx.Module):\n",
    "    \"\"\"\n",
    "    MiniGPT based Text Embedding Model.\n",
    "    Generates a fixed-size vector representation for input text.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, \n",
    "                 feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        \n",
    "        # 1. 임베딩 레이어 (동일)\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "            maxlen, vocab_size, embed_dim, rngs=rngs\n",
    "        )\n",
    "        \n",
    "        # 2. 트랜스포머 블록 스택 (동일)\n",
    "        self.transformer_blocks = [TransformerBlock(\n",
    "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
    "        ) for _ in range(num_transformer_blocks)]\n",
    "        \n",
    "        # [변경점 1] Output Layer(Linear to vocab_size)를 제거했습니다.\n",
    "        # 대신 임베딩 차원을 유지하거나, 특정 차원으로 줄이는 Projection Layer를 둘 수도 있습니다.\n",
    "        # 여기서는 Raw Hidden State를 그대로 사용합니다.\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        # 1. Embed inputs\n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "        # 2. Pass through Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "            \n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        # [변경점 2] Logits을 반환하지 않고, 마지막 Hidden State 전체를 반환합니다.\n",
    "        return x\n",
    "\n",
    "    @nnx.jit\n",
    "    def encode(self, inputs, attention_mask=None):\n",
    "        \"\"\"\n",
    "        입력 텍스트를 하나의 벡터로 변환(Pooling)하여 반환합니다.\n",
    "        여기서는 가장 널리 쓰이는 'Mean Pooling'을 구현합니다.\n",
    "        \"\"\"\n",
    "        # (Batch, Seq, Dim) 형태의 Hidden State 획득\n",
    "        hidden_states = self(inputs, training=False)\n",
    "        \n",
    "        # Mask가 없으면 패딩(0)이 아닌 부분만 1로 간주\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (inputs != 0).astype(jnp.float32)\n",
    "            \n",
    "        # 차원 확장: (Batch, Seq) -> (Batch, Seq, 1)\n",
    "        mask_expanded = attention_mask[:, :, None]\n",
    "        \n",
    "        # --- Mean Pooling 전략 ---\n",
    "        # 패딩이 아닌 토큰들의 벡터만 합산\n",
    "        sum_embeddings = jnp.sum(hidden_states * mask_expanded, axis=1)\n",
    "        # 패딩이 아닌 토큰의 개수 합산 (0으로 나누기 방지용 clamp)\n",
    "        sum_mask = jnp.clip(mask_expanded.sum(axis=1), a_min=1e-9)\n",
    "        \n",
    "        # 평균 계산\n",
    "        sentence_embedding = sum_embeddings / sum_mask\n",
    "        \n",
    "        # (Optional) L2 정규화: 코사인 유사도 계산을 쉽게 하기 위함\n",
    "        norm = jnp.linalg.norm(sentence_embedding, axis=1, keepdims=True)\n",
    "        return sentence_embedding / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(42)\n",
    "embedding_model = MiniGPTEmbedding(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks, rngs)\n",
    "\n",
    "# 3. 더미 데이터 (Batch size = 2, 문장 길이 = 32)\n",
    "sentence1 = tokenizer.encode(\"Today is Christmas\")\n",
    "sentence2 = tokenizer.encode(\"25 DEC is here\")\n",
    "input_ids = jnp.array([\n",
    "    sentence1 + [0] * (maxlen - len(sentence1)),\n",
    "    sentence2 + [0] * (maxlen - len(sentence2))\n",
    "])\n",
    "\n",
    "# 4. 임베딩 벡터 추출 (Encode)\n",
    "embeddings = embedding_model.encode(input_ids)\n",
    "\n",
    "print(f\"Embedding Shape: {embeddings.shape}\") \n",
    "# 출력: (2, 256) -> (배치 크기, 임베딩 차원)\n",
    "\n",
    "# 5. 코사인 유사도 계산 (벡터 내적)\n",
    "# 정규화가 되어 있으므로 내적(dot product)이 곧 코사인 유사도입니다.\n",
    "vec_a = embeddings[0]\n",
    "vec_b = embeddings[1]\n",
    "similarity = jnp.dot(vec_a, vec_b)\n",
    "\n",
    "print(f\"Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813cbf2",
   "metadata": {},
   "source": [
    "## Profiling for hyperparameter tuning\n",
    "\n",
    "**Note:** this section assume multiple TPU cores. Free-tier Colab TPU v5e-1 cannot run here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5fc4d",
   "metadata": {},
   "source": [
    "Load the tensorboard colab extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6131f",
   "metadata": {},
   "source": [
    "As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace/\"\n",
    "\n",
    "def loop_step(batch, step):\n",
    "    input_batch = jnp.array(jnp.array(batch).T)\n",
    "    target_batch = prep_target_batch(input_batch)\n",
    "    train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
    "\n",
    "def generate_trace():\n",
    "    tracing_steps = 30\n",
    "    warmup_steps = 5\n",
    "    for current_step in range(warmup_steps + tracing_steps):\n",
    "        if current_step == warmup_steps:\n",
    "            jax.profiler.start_trace(trace_dir)\n",
    "        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n",
    "            batch = next(text_dl)\n",
    "            loop_step(batch, current_step)\n",
    "\n",
    "    jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70f5b7",
   "metadata": {},
   "source": [
    "Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9452a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-batch-comparison/\"\n",
    "\n",
    "batch_size = 64\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()\n",
    "\n",
    "batch_size = 256\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea379965",
   "metadata": {},
   "source": [
    "Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 256`.\n",
    "\n",
    "The key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n",
    "\n",
    "In general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from 64 -> 256 achieves both of those. FLOPS increases from 16% to 27%. Average Step Time increase from 100ms to 260ms, however we increased our batch size by 300%. This means we move from 1.5ms per training example to 1.02ms per training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657967a5",
   "metadata": {},
   "source": [
    "Next, we can explore alternative parallelism methods. In cell #4, we used 4-way data parallel and 2-way tensor parallel. 8-way data parallel is another popular way. Let's compare results between them. To switch to 8-way data parallel, we'll replace the `Mesh` definition with:\n",
    "\n",
    "`mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))`\n",
    "\n",
    "JAX will automatically figure out how to shard the model and data to use the new partition strategy and nothing else need to be done. Re-connect the TPU runtime and run it again to see how it runs.\n",
    "\n",
    "How simple and powerful is this! And that's the beauty of JAX automatic parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-parallelism-comparison/\"\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "generate_trace()\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96e72b",
   "metadata": {},
   "source": [
    "Once again we'll run tensorboard.\n",
    "\n",
    "Looking at the results, we see that the step times are nearly the same, however the FLOPS Utilization is at 13% for 8-way data parallelism compared to 27% or 4-way data parallelism.\n",
    "\n",
    "By looking at the Trace Viewer tool and looking under each TPU's ops, we can see that the TPUs spend a large amount of time idle while waiting for the host, as well as spending a good amount of time in `reduce_sum` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca486e",
   "metadata": {},
   "source": [
    "By changing hyperparameters and comparing profiles, we're able to gain significant insights into our bottlenecks and limitations. These are just two examples of hyperparameters to tune, but plenty more of them will have significant effects on training speed and resource utilization."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
