{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a miniGPT language model with JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.kaggle.com/static/images/logos/kaggle-logo-transparent-300.png\" height=\"32\" width=\"70\"/>Run in Kaggle</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIOXoY1xgiww"
   },
   "source": [
    "This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for [Single-Program Multi-Data](https://en.wikipedia.org/wiki/Single_program,_multiple_data)). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
    "\n",
    "Here, you will learn how to:\n",
    "\n",
    "- Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "- Load and preprocess the dataset\n",
    "- Create the loss and training step functions\n",
    "- Train the model on TPUs on Kaggle or Google Colab\n",
    "- Profile for hyperparameter tuning\n",
    "\n",
    "If you are new to JAX for AI, check out the [introductory tutorial](https://jax-ai-stack.readthedocs.io/en/latest/neural_net_basics.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTmz5Cbco7n_"
   },
   "source": [
    "## Setup\n",
    "\n",
    "JAX installation is covered in [this guide](https://jax.readthedocs.io/en/latest/installation.html) on the JAX documentation site. We will use [Tiktoken](https://github.com/openai/tiktoken) for tokenization and [Grain](https://google-grain.readthedocs.io/en/latest/index.html) for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zMsOIc7ouCO",
    "outputId": "037d56a9-b18f-4504-f80a-3a4fa2945068"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq tiktoken jax-ai-stack[grain] matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rcji_799n4eA"
   },
   "source": [
    "**Note:** If you are using [Kaggle](https://www.kaggle.com/), select the free TPU v5e-8 as the hardware accelerator. If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v5e-1 as the hardware accelerator. You may also use Google Cloud TPUs.\n",
    "\n",
    "Check the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS9sQEY3n0mB",
    "outputId": "9ffcf3a6-20ef-4f80-b006-f5d3c5644a15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHzJ_bokoovZ"
   },
   "source": [
    "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUjQsgQEmI1N",
    "outputId": "e6eff24e-5578-4277-a0f9-24e27bd91ee0"
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKE2uUafLobI"
   },
   "source": [
    "Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MKYFNOhdLq98"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding # For data and model parallelism (explained in more detail later)\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import grain.python as pygrain\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPyt7MV6prz1"
   },
   "source": [
    "## Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "\n",
    "### Leveraging JAX's data and tensor parallelism\n",
    "\n",
    "One of the most powerful features of JAX is [device parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for SPMD.\n",
    "\n",
    "- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n",
    "- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n",
    "- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n",
    "\n",
    "In this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup, which is aligned with Kaggle TPU v5e-8 or newer GCP TPUs chips.\n",
    "\n",
    "Note that as of October 2025, free-tier Colab only offers TPU v5e-1, which can no longer support SPMD.\n",
    "\n",
    "### jax.sharding.Mesh\n",
    "\n",
    "Earlier, we imported [`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) - is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n",
    "\n",
    "Our `Mesh` will have two arguments:\n",
    "- `devices`: This will take the value of [`jax.experimental.mesh_utils((4, 2))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n",
    "- `axis_names`, where:\n",
    "  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n",
    "  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor parallism\n",
    "\n",
    "This matches the structure in the Kaggle TPU v5e setup.\n",
    "\n",
    "Let's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xuMlCK3Q8WJD"
   },
   "outputs": [],
   "source": [
    "# Create a `Mesh` object representing TPU device arrangement.\n",
    "# For example, for Kaggle TPU v5e-8:\n",
    "if jax.device_count() == 8:\n",
    "    mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "\n",
    "    ### Alternatively, we could use the 8-way data parallelism with only one line of code change.\n",
    "    ### JAX enables quick experimentation with different partitioning strategies\n",
    "    ### like this. We will come back to this point at the end of this tutorial.\n",
    "    # mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
    "\n",
    "### For free-tier Colab TPU, which only has a single TPU core\n",
    "if jax.device_count() == 1:\n",
    "    mesh = Mesh(mesh_utils.create_device_mesh((1, 1)), (\"batch\", \"model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZKdhNo98NgG"
   },
   "source": [
    "We will use the GPT-2 tokenizer from the [Tiktoken](https://github.com/openai/tiktoken) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iWbkk1V7-Isg"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XHQ0BQ9-KIj"
   },
   "source": [
    "To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. Earlier, we also imported [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) and [`jax.sharding.NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding):\n",
    "- [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) (using alias `P`) defines how tensors are sharded across the devices in our `Mesh`. Its elements describe how an input dimension is partitioned across mesh dimensions. For example, in `PartitionSpec('x', 'y')` the first dimension of data is sharded across `x` axis of the mesh, and the second one - across the `y` axis.\n",
    "  - We'll use `PartitionSpec` to describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n",
    "- [`NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding) is a (`Mesh`, `PartitionSpec`) pair that describes how to shard a model tensor across our `mesh`.\n",
    "- We combine `Mesh` (the TPU resources) with `PartitionSpec` and create a `NamedSharding`, which instructs how to shard each model tensor across the TPU devices.\n",
    "\n",
    "Additionally, we'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n",
    "- `nnx.with_partitioning` will take two arguments, such as the `initializer` (such as [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)) and `sharding` (e.g. `NamedSharding(Mesh, PartitionSpec)` or `NamedSharding(mesh, P('model')` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z0p-IHurrB9i"
   },
   "outputs": [],
   "source": [
    "# Define a triangular mask for causal attention with `jax.numpy.tril` and `jax.numpy.ones`.\n",
    "def causal_attention_mask(seq_len):\n",
    "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    \"\"\" A single Transformer block.\n",
    "\n",
    "    Each Transformer block processes input sequences via self-attention and feed-forward networks.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        ff_dim (int): Dimensionality of the feed-forward network.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "        rate (float): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
    "        # Multi-Head Attention (MHA) with `flax.nnx.MultiHeadAttention`.\n",
    "        # Specifies tensor sharding (depending on the mesh configuration)\n",
    "        # where we shard the weights across devices for parallel computation.\n",
    "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
    "                                          in_features=embed_dim,\n",
    "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n",
    "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                          rngs=rngs)\n",
    "        # The first dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout1 = nnx.Dropout(rate=rate)\n",
    "        # First layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                         rngs=rngs)\n",
    "        # The first linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
    "                                  out_features=ff_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                  rngs=rngs)\n",
    "        # The second linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
    "                                  out_features=embed_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                  rngs=rngs)\n",
    "        # The second dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout2 = nnx.Dropout(rate=rate)\n",
    "        # Second layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                         rngs=rngs)\n",
    "\n",
    "    # Apply the Transformer block to the input sequence.\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        input_shape = inputs.shape\n",
    "        _, seq_len, _ = input_shape\n",
    "\n",
    "        # Instantiate the causal attention mask.\n",
    "        mask = causal_attention_mask(seq_len)\n",
    "\n",
    "        # Apply Multi-Head Attention with the causal attention mask.\n",
    "        attention_output = self.mha(\n",
    "            inputs_q=inputs,\n",
    "            mask=mask,\n",
    "            decode=False\n",
    "        )\n",
    "        # Apply the first dropout.\n",
    "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
    "        # Apply the first layer normalization.\n",
    "        out1 = self.layer_norm1(inputs + attention_output)\n",
    "\n",
    "        # The feed-forward network.\n",
    "        # Apply the first linear transformation.\n",
    "        ffn_output = self.linear1(out1)\n",
    "        # Apply the ReLU activation with `flax.nnx.relu`.\n",
    "        ffn_output = nnx.relu(ffn_output)\n",
    "        # Apply the second linear transformation.\n",
    "        ffn_output = self.linear2(ffn_output)\n",
    "        # Apply the second dropout.\n",
    "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
    "        # Apply the second layer normalization and return the output of the Transformer block.\n",
    "        return self.layer_norm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(nnx.Module):\n",
    "    \"\"\" Combines token embeddings (words in an input sentence) with\n",
    "    positional embeddings (the position of each word in a sentence).\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Matimum sequence length.\n",
    "        vocal_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
    "        # Initialize token embeddings (using `flax.nnx.Embed`).\n",
    "        # Each unique word has an embedding vector.\n",
    "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
    "        # Initialize positional embeddings (using `flax.nnx.Embed`).\n",
    "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
    "\n",
    "    # Takes a token sequence (integers) and returns the combined token and positional embeddings.\n",
    "    def __call__(self, x):\n",
    "        # Generate a sequence of positions for the input tokens.\n",
    "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
    "        # Look up the positional embeddings for each position in the input sequence.\n",
    "        position_embedding = self.pos_emb(positions)\n",
    "        # Look up the token embeddings for each token in the input sequence.\n",
    "        token_embedding = self.token_emb(x)\n",
    "        # Combine token and positional embeddings.\n",
    "        return token_embedding + position_embedding\n",
    "\n",
    "class MiniGPT(nnx.Module, pytree=False):\n",
    "    \"\"\" A miniGPT transformer model, inherits from `flax.nnx.Module`.\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Maximum sequence length.\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        feed_forward_dim (int): Dimensionality of the feed-forward network.\n",
    "        num_transformer_blocks (int): Number of transformer blocks. Each block contains attention and feed-forward networks.\n",
    "        rngs (nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    # Initialize miniGPT model components.\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        # Initiliaze the `TokenAndPositionEmbedding` that combines token and positional embeddings.\n",
    "        print(\"Initializing\")\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
    "                )\n",
    "        # Create a list of `TransformerBlock` instances.\n",
    "        # Each block processes input sequences using attention and feed-forward networks.\n",
    "        print(\"Transformer block\")\n",
    "        self.transformer_blocks = [TransformerBlock(\n",
    "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
    "        ) for _ in range(num_transformer_blocks)]\n",
    "        # Initialize the output `flax.nnx.Linear` layer producing logits over the vocabulary for next-token prediction.\n",
    "        print(\"Output layer\")\n",
    "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
    "                                       out_features=vocab_size,\n",
    "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P('model')),\n",
    "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "                                       rngs=rngs)\n",
    "        print(\"Done\")\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        # Pass the input tokens through the `embedding_layer` to get token embeddings.\n",
    "        # Apply each transformer block sequentially to the embedded input, use the `training` flag for the behavior of `flax.nnx.Dropout`.\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        # Pass the output of the transformer blocks through the output layer,\n",
    "        # and obtain logits for each token in the vocabulary (for next token prediction).\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    @nnx.jit\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "        logits = nnx.softmax(logits)\n",
    "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
    "\n",
    "    @nnx.jit\n",
    "    def generate_step(self, padded_tokens, sample_index):\n",
    "        logits = self(padded_tokens)\n",
    "        next_token = self.sample_from(logits[0][sample_index])\n",
    "        return next_token\n",
    "\n",
    "    def generate_text(self, max_tokens, start_tokens):\n",
    "        generated = []\n",
    "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
    "        for i in range(max_tokens):\n",
    "            sample_index = len(start_tokens) + len(generated) - 1\n",
    "\n",
    "            padded_tokens = jnp.array((start_tokens + generated + [0] * (maxlen - len(start_tokens) - len(generated))))[None, :]\n",
    "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
    "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
    "              break\n",
    "            generated.append(next_token)\n",
    "            # decode and print next_token\n",
    "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
    "        return tokenizer.decode(start_tokens + generated)\n",
    "\n",
    "# Creates the miniGPT model with 4 transformer blocks.\n",
    "def create_model(rngs):\n",
    "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GRhiDsCrMZRp"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "num_transformer_blocks = 8\n",
    "maxlen = 256\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "feed_forward_dim = 256\n",
    "batch_size = 144 * jax.device_count() / 2  # divide by 2 in case of model parallelism\n",
    "if jax.device_count() == 1:\n",
    "    batch_size = 144\n",
    "num_epochs = 1\n",
    "top_k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igX_eoGNMTGR"
   },
   "source": [
    "Set some hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI1ci-HyMspJ"
   },
   "source": [
    "## Loading and preprocessing the data\n",
    "\n",
    "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rGUFsn1GMuzh"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextDataset:\n",
    "    data: list\n",
    "    maxlen: int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Use Tiktoken for tokenization\n",
    "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
    "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
    "\n",
    "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "      text = f.read()\n",
    "\n",
    "    stories = text.split('<|endoftext|>')\n",
    "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
    "    df = pd.DataFrame({'text': stories})\n",
    "    data = df['text'].dropna().tolist()\n",
    "    dataset = TextDataset(data, maxlen)\n",
    "\n",
    "    sampler = pygrain.IndexSampler(\n",
    "        len(dataset),\n",
    "        shuffle=False,\n",
    "        seed=42,\n",
    "        shard_options=pygrain.NoSharding(),\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    dl = pygrain.DataLoader(\n",
    "        data_source=dataset,\n",
    "        sampler=sampler,\n",
    "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
    "    )\n",
    "\n",
    "    return dl\n",
    "\n",
    "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKVSD8KSM1um"
   },
   "source": [
    "## Defining the loss function and training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8rRuTmABNV4b"
   },
   "outputs": [],
   "source": [
    "# Defines the loss function using `optax.softmax_cross_entropy_with_integer_labels`.\n",
    "def loss_fn(model, batch):\n",
    "    logits = model(batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, logits\n",
    "\n",
    "# Define the training step with the `flax.nnx.jit` transformation decorator.\n",
    "@nnx.jit\n",
    "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
    "    optimizer.update(model, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5um2vkeUNckm"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Start training. It takes ~50 minutes on Colab.\n",
    "\n",
    "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put` with `NamedSharding`.\n",
    "\n",
    "We are also using the `jax.vmap` transformation to produce the target sequences faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ysl6CsfENeJN",
    "outputId": "5dd06dca-f030-4927-a9b6-35d412da535c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Transformer block\n",
      "Output layer\n",
      "Done\n",
      "Initial generated text:\n",
      "Once upon a timeaciaGender gearuser Analysisval {} Bruce Lauren helic Lauren Bruce againstliterally SQU retire Path {}valascript northwest {} Bruceuit Pathascript northwestdrops freelyvic996 curated hysteria FS psychedelic {}ligaval {} perished {} Woman {} {} inflicwaves Analysis996 servants Bruce Charlotte dissatisfiedascript Whites retire {} retire {} perished Schools {}Nick sorrow psychedelicfingerelaide {} perished Blackburnrestyy elder swing interesting Tang Shattered {} {}scl less abnorm abnorm Goldmancre motions fleshthird fatal haw {} perished {} Dani Whites {} Whites retire {} sorrowolate Whites {} Womanlast doom appendixfinger {}Rain pressurefinger {} {} Womanorem Bruce simplyrest {} {} inexplicablerest HS Charlotte dissatisfied stranger perished Tang reassUV {} Abortion outragedaligned {} server fertility decision FSShould {} {} northwestroid variable {} Whites {} dancersithmetic {} {} Whites las hampered {} Woman {} Woman {} Mineundrum {} inexplicable taking Keithrest {} {} retire interesting Tangresterv beyond THR Mae CaptainUV {}sumthird psychedelic {} acidicundrum salesmanundrumOSE lasManager twins Pathrest {}urnedrestrisis {} Runtime {} perished485 Analysis developerscre Atom {}scl HouthInteger {} northwest appease miles {} perished THR Hyundai Captainilipp {} inflic decision {} inflic {} retire {} Whites dancers {}scl FS lore appease Din {} Whites hidingBT {} {}val {} specialist enchantExcept pressure psychedelicsteps tell hiding!!!\n",
      "\n",
      "Step 200, Loss: 4.7713518142700195, Elapsed Time: 30.36 seconds\n",
      "Generated text:\n",
      "Once upon a time a a a a a little a little a a little a little a little little a little a little a little a little a little a a a little a little a little a a little loved... She a play... She a play.. She a play... She a play.... She a big big play.. She a big big big play.\n",
      "\n",
      "\n",
      "Step 400, Loss: 3.675863265991211, Elapsed Time: 10.14 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a little girl named named named named named named named named named named named Lily was very very very play. She was very play, she was so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so play to play to play to play, she was so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so!!!!\n",
      "\n",
      "Step 600, Loss: 3.1762349605560303, Elapsed Time: 11.35 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a little girl named Lily. She loved to play with a big. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was very excited. She was so excited. She was very excited. She was so excited. She was so excited. She was so excited. She was so excited. She was so excited. She was so excited. She was so excited. She was so happy. She was so happy. She was so happy. She was so happy. She was so happy and the park and the park and the park.\n",
      "\n",
      "\n",
      "Step 800, Loss: 2.7927541732788086, Elapsed Time: 10.67 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a little girl named Timmy. He loved to play with his toys and his toys and his toys. One day, Timmy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy and daddy.\n",
      "\n",
      "\n",
      "Step 1000, Loss: 2.539199113845825, Elapsed Time: 10.87 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She was very happy to her mommy and daddy were going to the store to buy a new toy. Lily was so excited and daddy went to the store to buy a new toy. Lily was so she asked her mommy if she was so she was so she was so excited. She asked her mommy if she was a new friend, but she was so she was so she was so she was so she was so she was so she was so she was so she was so excited.\n",
      "When she was done, she went to her mommy and she was so she was so she was so excited to go to the store. She was so excited to her mommy and daddy was so excited to her mommy and daddy. She was so excited to her mommy and daddy had to her mommy and daddy.\n",
      "Final generated text:\n",
      "Once upon a time there was a little girl named She was very adventurous girl. She loved to play with her friends. One day, she went to the park and saw a big tree. She wanted to play with it, but she was too far away. She tried to climb up the tree and tried to catch it.\n",
      "The little girl was sad and tried to help. She tried to help but she was too fast. She tried to help her. She tried to help her. She tried to help her. She tried to help her. She tried to help her.\n",
      "The little girl was sad and said, \"I'm sorry, I'm sorry, I'm sorry I'm sorry I will help you help you help you help you.\"\n",
      "The little girl was sad and said, \"I'm sorry I'm sorry I will help you.\"\n",
      "The little girl was happy and said, \"I'm sorry, I will help you. I will help you.\"\n",
      "The little girl was happy and said, \"Thank you, I will help you.\"\n"
     ]
    }
   ],
   "source": [
    "jax.set_mesh(mesh)\n",
    "model = create_model(rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n",
    "metrics = nnx.MultiMetric(\n",
    "    loss=nnx.metrics.Average(\"loss\"),\n",
    ")\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "start_prompt = \"Once upon a time\"\n",
    "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
    "print(\"Initial generated text:\")\n",
    "generated_text = model.generate_text(maxlen, start_tokens)\n",
    "\n",
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "}\n",
    "\n",
    "prep_target_batch = jax.vmap(\n",
    "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
    ")\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for batch in text_dl:\n",
    "        if len(batch) % len(jax.devices()) != 0:\n",
    "            continue  # skip the remaining elements\n",
    "        input_batch = jnp.array(jnp.array(batch).T)\n",
    "        target_batch = prep_target_batch(input_batch)\n",
    "        train_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            metrics,\n",
    "            jax.device_put(\n",
    "                (input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if (step + 1) % 200 == 0:\n",
    "            for metric, value in metrics.compute().items():\n",
    "                metrics_history[f\"train_{metric}\"].append(value)\n",
    "            metrics.reset()\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(\n",
    "                f\"\\n\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\"\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "            print(\"Generated text:\")\n",
    "            generated_text = model.generate_text(maxlen, start_tokens)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "# Final text generation\n",
    "print(\"Final generated text:\")\n",
    "generated_text = model.generate_text(maxlen, start_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thaLs6TD0lt5"
   },
   "source": [
    "Visualize the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "B6Eg1Cz2y_iP",
    "outputId": "7cafe711-1ae4-4eb9-fd37-e1bde54cbfc5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(metrics_history['train_loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB-ExEt1Zl1C"
   },
   "source": [
    "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 27,430,481\n"
     ]
    }
   ],
   "source": [
    "#  (State) \n",
    "state = nnx.state(model)\n",
    "\n",
    "#  (Leaf )  \n",
    "total_params = sum(x.size for x in jax.tree_util.tree_leaves(state))\n",
    "print(f\"Total Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_layer.pos_emb.embedding                            | (256, 256)           | 65,536         \n",
      "embedding_layer.token_emb.embedding                          | (50257, 256)         | 12,865,792     \n",
      "output_layer.bias                                            | (50257,)             | 50,257         \n",
      "output_layer.kernel                                          | (256, 50257)         | 12,865,792     \n",
      "transformer_blocks.0.layer_norm1.bias                        | (256,)               | 256            \n",
      "transformer_blocks.0.layer_norm1.scale                       | (256,)               | 256            \n",
      "transformer_blocks.0.layer_norm2.bias                        | (256,)               | 256            \n",
      "transformer_blocks.0.layer_norm2.scale                       | (256,)               | 256            \n",
      "transformer_blocks.0.linear1.bias                            | (256,)               | 256            \n",
      "transformer_blocks.0.linear1.kernel                          | (256, 256)           | 65,536         \n",
      "transformer_blocks.0.linear2.bias                            | (256,)               | 256            \n",
      "transformer_blocks.0.linear2.kernel                          | (256, 256)           | 65,536         \n",
      "transformer_blocks.0.mha.key.bias                            | (8, 32)              | 256            \n",
      "transformer_blocks.0.mha.key.kernel                          | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.0.mha.out.bias                            | (256,)               | 256            \n",
      "transformer_blocks.0.mha.out.kernel                          | (8, 32, 256)         | 65,536         \n",
      "transformer_blocks.0.mha.query.bias                          | (8, 32)              | 256            \n",
      "transformer_blocks.0.mha.query.kernel                        | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.0.mha.value.bias                          | (8, 32)              | 256            \n",
      "transformer_blocks.0.mha.value.kernel                        | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.1.layer_norm1.bias                        | (256,)               | 256            \n",
      "transformer_blocks.1.layer_norm1.scale                       | (256,)               | 256            \n",
      "transformer_blocks.1.layer_norm2.bias                        | (256,)               | 256            \n",
      "transformer_blocks.1.layer_norm2.scale                       | (256,)               | 256            \n",
      "transformer_blocks.1.linear1.bias                            | (256,)               | 256            \n",
      "transformer_blocks.1.linear1.kernel                          | (256, 256)           | 65,536         \n",
      "transformer_blocks.1.linear2.bias                            | (256,)               | 256            \n",
      "transformer_blocks.1.linear2.kernel                          | (256, 256)           | 65,536         \n",
      "transformer_blocks.1.mha.key.bias                            | (8, 32)              | 256            \n",
      "transformer_blocks.1.mha.key.kernel                          | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.1.mha.out.bias                            | (256,)               | 256            \n",
      "transformer_blocks.1.mha.out.kernel                          | (8, 32, 256)         | 65,536         \n",
      "transformer_blocks.1.mha.query.bias                          | (8, 32)              | 256            \n",
      "transformer_blocks.1.mha.query.kernel                        | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.1.mha.value.bias                          | (8, 32)              | 256            \n",
      "transformer_blocks.1.mha.value.kernel                        | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.2.layer_norm1.bias                        | (256,)               | 256            \n",
      "transformer_blocks.2.layer_norm1.scale                       | (256,)               | 256            \n",
      "transformer_blocks.2.layer_norm2.bias                        | (256,)               | 256            \n",
      "transformer_blocks.2.layer_norm2.scale                       | (256,)               | 256            \n",
      "transformer_blocks.2.linear1.bias                            | (256,)               | 256            \n",
      "transformer_blocks.2.linear1.kernel                          | (256, 256)           | 65,536         \n",
      "transformer_blocks.2.linear2.bias                            | (256,)               | 256            \n",
      "transformer_blocks.2.linear2.kernel                          | (256, 256)           | 65,536         \n",
      "transformer_blocks.2.mha.key.bias                            | (8, 32)              | 256            \n",
      "transformer_blocks.2.mha.key.kernel                          | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.2.mha.out.bias                            | (256,)               | 256            \n",
      "transformer_blocks.2.mha.out.kernel                          | (8, 32, 256)         | 65,536         \n",
      "transformer_blocks.2.mha.query.bias                          | (8, 32)              | 256            \n",
      "transformer_blocks.2.mha.query.kernel                        | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.2.mha.value.bias                          | (8, 32)              | 256            \n",
      "transformer_blocks.2.mha.value.kernel                        | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.3.layer_norm1.bias                        | (256,)               | 256            \n",
      "transformer_blocks.3.layer_norm1.scale                       | (256,)               | 256            \n",
      "transformer_blocks.3.layer_norm2.bias                        | (256,)               | 256            \n",
      "transformer_blocks.3.layer_norm2.scale                       | (256,)               | 256            \n",
      "transformer_blocks.3.linear1.bias                            | (256,)               | 256            \n",
      "transformer_blocks.3.linear1.kernel                          | (256, 256)           | 65,536         \n",
      "transformer_blocks.3.linear2.bias                            | (256,)               | 256            \n",
      "transformer_blocks.3.linear2.kernel                          | (256, 256)           | 65,536         \n",
      "transformer_blocks.3.mha.key.bias                            | (8, 32)              | 256            \n",
      "transformer_blocks.3.mha.key.kernel                          | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.3.mha.out.bias                            | (256,)               | 256            \n",
      "transformer_blocks.3.mha.out.kernel                          | (8, 32, 256)         | 65,536         \n",
      "transformer_blocks.3.mha.query.bias                          | (8, 32)              | 256            \n",
      "transformer_blocks.3.mha.query.kernel                        | (256, 8, 32)         | 65,536         \n",
      "transformer_blocks.3.mha.value.bias                          | (8, 32)              | 256            \n",
      "transformer_blocks.3.mha.value.kernel                        | (256, 8, 32)         | 65,536         \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Total Parameters: 27,430,481\n"
     ]
    }
   ],
   "source": [
    "state = nnx.state(model)\n",
    "flat_params = state.flat_state()\n",
    "total_params = 0\n",
    "# items() () ( ) \n",
    "for path_tuple, param in flat_params:\n",
    "    # 1.     (: ('linear1', 'kernel') -> 'linear1.kernel')\n",
    "    path_str = \".\".join(str(p) for p in path_tuple)\n",
    "    \n",
    "    # 2.  (Shape)  (Size) \n",
    "    # param nnx.Variable  jax.Array    .value      \n",
    "    tensor = param.value if hasattr(param, 'value') else param\n",
    "    shape = tensor.shape\n",
    "    count = tensor.size\n",
    "    \n",
    "    # 3.   \n",
    "    total_params += count\n",
    "    \n",
    "    # 4. \n",
    "    print(f\"{path_str:<60} | {str(shape):<20} | {count:<15,}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"Total Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soPqiR1JNmjf"
   },
   "source": [
    "## Saving the checkpoint\n",
    "\n",
    "Save the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkoFGCgSZ1yz",
    "outputId": "3467b8ba-ce05-42f0-fb89-75922cc91e31"
   },
   "outputs": [],
   "source": [
    "import orbax.checkpoint as orbax\n",
    "\n",
    "dir = \"/home/ext_jeehyeok_google_com/save\"\n",
    "state = nnx.state(model)\n",
    "\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "checkpointer.save(dir, args=orbax.args.PyTreeSave(state), force=True)\n",
    "\n",
    "# Make sure the files are there\n",
    "!ls {dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "#  TransformerBlock, TokenAndPositionEmbedding, causal_attention_mask  .\n",
    "# (    )\n",
    "\n",
    "class MiniGPTEmbedding(nnx.Module, pytree=False):\n",
    "    \"\"\"\n",
    "    MiniGPT based Text Embedding Model.\n",
    "    Generates a fixed-size vector representation for input text.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, \n",
    "                 feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        \n",
    "        # 1.   ()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "            maxlen, vocab_size, embed_dim, rngs=rngs\n",
    "        )\n",
    "        \n",
    "        # 2.    ()\n",
    "        self.transformer_blocks = [TransformerBlock(\n",
    "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
    "        ) for _ in range(num_transformer_blocks)]\n",
    "        \n",
    "        # [ 1] Output Layer(Linear to vocab_size) .\n",
    "        #    ,    Projection Layer   .\n",
    "        #  Raw Hidden State  .\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        # 1. Embed inputs\n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "        # 2. Pass through Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "            \n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        # [ 2] Logits  ,  Hidden State  .\n",
    "        return x\n",
    "\n",
    "    @nnx.jit\n",
    "    def encode(self, inputs, attention_mask=None):\n",
    "        \"\"\"\n",
    "            (Pooling) .\n",
    "            'Mean Pooling' .\n",
    "        \"\"\"\n",
    "        # (Batch, Seq, Dim)  Hidden State \n",
    "        hidden_states = self(inputs, training=False)\n",
    "        \n",
    "        # Mask  (0)   1 \n",
    "        if attention_mask is None:\n",
    "            attention_mask = (inputs != 0).astype(jnp.float32)\n",
    "            \n",
    "        #  : (Batch, Seq) -> (Batch, Seq, 1)\n",
    "        mask_expanded = attention_mask[:, :, None]\n",
    "        \n",
    "        # --- Mean Pooling  ---\n",
    "        #     \n",
    "        sum_embeddings = jnp.sum(hidden_states * mask_expanded, axis=1)\n",
    "        #      (0   clamp)\n",
    "        sum_mask = jnp.clip(mask_expanded.sum(axis=1), a_min=1e-9)\n",
    "        \n",
    "        #  \n",
    "        sentence_embedding = sum_embeddings / sum_mask\n",
    "        \n",
    "        # (Optional) L2 :      \n",
    "        norm = jnp.linalg.norm(sentence_embedding, axis=1, keepdims=True)\n",
    "        return sentence_embedding / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Shape: (2, 256)\n",
      "Similarity: 0.7332\n"
     ]
    }
   ],
   "source": [
    "rngs = nnx.Rngs(42)\n",
    "embedding_model = MiniGPTEmbedding(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks, rngs)\n",
    "\n",
    "# 3.   (Batch size = 2,   = 32)\n",
    "sentence1 = tokenizer.encode(\"Today is Christmas\")\n",
    "sentence2 = tokenizer.encode(\"25 DEC is here\")\n",
    "input_ids = jnp.array([\n",
    "    sentence1 + [0] * (maxlen - len(sentence1)),\n",
    "    sentence2 + [0] * (maxlen - len(sentence2))\n",
    "])\n",
    "\n",
    "# 4.    (Encode)\n",
    "embeddings = embedding_model.encode(input_ids)\n",
    "\n",
    "print(f\"Embedding Shape: {embeddings.shape}\") \n",
    "# : (2, 256) -> ( ,  )\n",
    "\n",
    "# 5.    ( )\n",
    "#    (dot product)   .\n",
    "vec_a = embeddings[0]\n",
    "vec_b = embeddings[1]\n",
    "similarity = jnp.dot(vec_a, vec_b)\n",
    "\n",
    "print(f\"Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813cbf2",
   "metadata": {},
   "source": [
    "## Profiling for hyperparameter tuning\n",
    "\n",
    "**Note:** this section assume multiple TPU cores. Free-tier Colab TPU v5e-1 cannot run here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5fc4d",
   "metadata": {},
   "source": [
    "Load the tensorboard colab extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6131f",
   "metadata": {},
   "source": [
    "As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace/\"\n",
    "\n",
    "def loop_step(batch, step):\n",
    "    input_batch = jnp.array(jnp.array(batch).T)\n",
    "    target_batch = prep_target_batch(input_batch)\n",
    "    train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
    "\n",
    "def generate_trace():\n",
    "    tracing_steps = 30\n",
    "    warmup_steps = 5\n",
    "    for current_step in range(warmup_steps + tracing_steps):\n",
    "        if current_step == warmup_steps:\n",
    "            jax.profiler.start_trace(trace_dir)\n",
    "        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n",
    "            batch = next(text_dl)\n",
    "            loop_step(batch, current_step)\n",
    "\n",
    "    jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70f5b7",
   "metadata": {},
   "source": [
    "Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9452a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-batch-comparison/\"\n",
    "\n",
    "batch_size = 64\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()\n",
    "\n",
    "batch_size = 256\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea379965",
   "metadata": {},
   "source": [
    "Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 256`.\n",
    "\n",
    "The key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n",
    "\n",
    "In general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from 64 -> 256 achieves both of those. FLOPS increases from 16% to 27%. Average Step Time increase from 100ms to 260ms, however we increased our batch size by 300%. This means we move from 1.5ms per training example to 1.02ms per training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657967a5",
   "metadata": {},
   "source": [
    "Next, we can explore alternative parallelism methods. In cell #4, we used 4-way data parallel and 2-way tensor parallel. 8-way data parallel is another popular way. Let's compare results between them. To switch to 8-way data parallel, we'll replace the `Mesh` definition with:\n",
    "\n",
    "`mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))`\n",
    "\n",
    "JAX will automatically figure out how to shard the model and data to use the new partition strategy and nothing else need to be done. Re-connect the TPU runtime and run it again to see how it runs.\n",
    "\n",
    "How simple and powerful is this! And that's the beauty of JAX automatic parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-parallelism-comparison/\"\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "generate_trace()\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96e72b",
   "metadata": {},
   "source": [
    "Once again we'll run tensorboard.\n",
    "\n",
    "Looking at the results, we see that the step times are nearly the same, however the FLOPS Utilization is at 13% for 8-way data parallelism compared to 27% or 4-way data parallelism.\n",
    "\n",
    "By looking at the Trace Viewer tool and looking under each TPU's ops, we can see that the TPUs spend a large amount of time idle while waiting for the host, as well as spending a good amount of time in `reduce_sum` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca486e",
   "metadata": {},
   "source": [
    "By changing hyperparameters and comparing profiles, we're able to gain significant insights into our bottlenecks and limitations. These are just two examples of hyperparameters to tune, but plenty more of them will have significant effects on training speed and resource utilization."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
